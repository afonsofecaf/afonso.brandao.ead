{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula: Gradiente e Otimização - O Papel da Derivada e da Integral\n",
    "\n",
    "## Introdução\n",
    "O gradiente e os métodos de otimização são fundamentais para o aprendizado de modelos matemáticos, redes neurais e inteligência artificial. A base matemática dessas técnicas vem do **cálculo diferencial e integral**, que nos permite compreender como uma função muda e como podemos minimizar seu erro.\n",
    "\n",
    "## 1. O Papel da Derivada na Otimização\n",
    "A **derivada** mede a taxa de variação instantânea de uma função. Em outras palavras, ela nos diz quão rápido uma função está crescendo ou diminuindo em um determinado ponto.\n",
    "\n",
    "Se tivermos uma função \\( f(x) \\), sua derivada é dada por:\n",
    "\n",
    "$$ f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h} $$\n",
    "\n",
    "A derivada é crucial na otimização porque permite encontrar **pontos críticos** de uma função, ou seja, onde sua inclinação é zero, indicando um **máximo ou mínimo local**.\n",
    "\n",
    "### Exemplo: Derivada de uma Função Quadrática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivada de f: 2*x\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "f = x**2  # Função quadrática\n",
    "f_deriv = sp.diff(f, x)\n",
    "print(\"Derivada de f:\", f_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso indica que o gradiente da função é **2x**, mostrando como a função varia em diferentes valores de \\( x \\).\n",
    "\n",
    "## 2. A Integral e o Acúmulo de Mudanças\n",
    "Enquanto a derivada nos diz **como uma função muda**, a **integral** nos informa o **acúmulo** dessas mudanças ao longo de um intervalo. Na otimização, a integral é usada para entender o comportamento global da função e em técnicas como **regularização**.\n",
    "\n",
    "A integral definida de uma função \\( f(x) \\) no intervalo \\([a, b]\\) é:\n",
    "\n",
    "$$ \\int_a^b f(x)dx $$\n",
    "\n",
    "### Exemplo: Cálculo da Integral de uma Função Quadrática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Área sob a curva de f(x) de 0 a 2: 8/3\n"
     ]
    }
   ],
   "source": [
    "integral = sp.integrate(f, (x, 0, 2))\n",
    "print(\"Área sob a curva de f(x) de 0 a 2:\", integral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso nos dá a soma acumulada da função no intervalo dado.\n",
    "\n",
    "## 3. Como a Derivada e a Integral Relacionam-se com a Descida do Gradiente\n",
    "A **descida do gradiente** é um método de otimização baseado na derivada para encontrar os mínimos locais de uma função. Ele segue a equação:\n",
    "\n",
    "$$ x_{novo} = x_{atual} - \\alpha f'(x) $$\n",
    "\n",
    "Onde:\n",
    "- \\( \\alpha \\) é a **taxa de aprendizado** (define o tamanho do passo da atualização).\n",
    "- \\( f'(x) \\) é a **derivada da função**.\n",
    "\n",
    "A derivada permite que a descida do gradiente saiba **em que direção mover-se** para minimizar a função.\n",
    "\n",
    "### Implementação da Descida do Gradiente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mínimo encontrado: 1.2433766019241796e-107\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(start_x, learning_rate=0.11, iterations=1000):\n",
    "    x = start_x\n",
    "    for _ in range(iterations):\n",
    "        grad = 2 * x  # Derivada de x^2 é 2x\n",
    "        x -= learning_rate * grad\n",
    "    return x\n",
    "\n",
    "resultado = gradient_descent(10)\n",
    "print(\"Mínimo encontrado:\", resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso mostra que a descida do gradiente nos levou ao **mínimo da função**.\n",
    "\n",
    "## 4. O Papel da Integral na Regularização\n",
    "Em alguns problemas de otimização, buscamos minimizar não apenas o erro da previsão, mas também penalizar a complexidade do modelo. Esse processo é feito com **regularização**, que usa a **integral do quadrado dos pesos** para suavizar a função de custo:\n",
    "\n",
    "$$ L(w) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\int w^2 dx $$\n",
    "\n",
    "Isso impede que a função aprenda padrões errôneos devido ao **overfitting**.\n",
    "\n",
    "### Implementação da Regularização com Integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularização da função de custo: w**3/3\n"
     ]
    }
   ],
   "source": [
    "w = sp.Symbol('w')\n",
    "regularization = sp.integrate(w**2, w)\n",
    "print(\"Regularização da função de custo:\", regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "O **cálculo diferencial e integral** é a base da otimização e da descida do gradiente:\n",
    "- A **derivada** nos diz como uma função muda, permitindo encontrar mínimos e máximos.\n",
    "- A **integral** nos dá o acúmulo de mudanças, ajudando na regularização e suavização do modelo.\n",
    "- A **descida do gradiente** usa a derivada para encontrar a melhor solução iterativamente.\n",
    "\n",
    "Compreender esses conceitos é essencial para o desenvolvimento de algoritmos eficientes e modelos de aprendizado de máquina mais robustos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
