{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula: Aplicação de Cálculo em Redes Neurais\n",
    "\n",
    "## Introdução\n",
    "As redes neurais são modelos matemáticos inspirados no funcionamento do cérebro humano. A base teórica para o treinamento de redes neurais depende fortemente do **cálculo diferencial e integral**, utilizado para otimização dos pesos da rede e melhoria das previsões do modelo.\n",
    "\n",
    "## 1. Funções de Ativação e Derivadas\n",
    "As funções de ativação são essenciais para introduzir **não-linearidade** em redes neurais. Elas permitem que o modelo aprenda padrões complexos e realizem tarefas como classificação e regressão. \n",
    "\n",
    "### Exemplo: Função Sigmoide\n",
    "A função sigmoide é definida como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.2350037122015945)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_derivative(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A derivada da sigmoide é utilizada para calcular o **gradiente** durante o treinamento da rede neural.\n",
    "\n",
    "## 2. Descida do Gradiente\n",
    "A descida do gradiente é um método de otimização baseado no **cálculo diferencial** que ajusta os pesos da rede neural para minimizar uma função de perda.\n",
    "\n",
    "A atualização dos pesos segue a regra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w, grad, learning_rate=0.01):\n",
    "    return w - learning_rate * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onde:\n",
    "- `w` são os pesos da rede\n",
    "- `grad` é o gradiente da função de perda\n",
    "- `learning_rate` é a taxa de aprendizado\n",
    "\n",
    "## 3. Processamento de Dados e Transformada de Fourier\n",
    "Muitos modelos de redes neurais trabalham com **sinais**, como áudio e imagem. A **Transformada de Fourier** é usada para decompor um sinal no domínio do tempo em suas frequências fundamentais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "\n",
    "def fourier_transform(signal):\n",
    "    return fft(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa técnica é fundamental para o **reconhecimento de padrões em sinais**, como áudio e texto, dentro de redes neurais.\n",
    "\n",
    "## 4. Construindo uma Rede Neural Simples\n",
    "Abaixo, implementamos uma **rede neural simples** com uma camada oculta e uma função de ativação sigmoide, enfatizando os cálculos matemáticos envolvidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inicialização de pesos aleatória\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    W1 = np.random.randn(input_size, hidden_size)\n",
    "    W2 = np.random.randn(hidden_size, output_size)\n",
    "    return W1, W2\n",
    "\n",
    "# Forward Pass\n",
    "def forward(X, W1, W2):\n",
    "    Z1 = np.dot(X, W1)\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2)\n",
    "    A2 = sigmoid(Z2)\n",
    "    return A2\n",
    "\n",
    "# Cálculo do erro (MSE)\n",
    "def compute_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Backpropagation\n",
    "def backward(X, y_true, W1, W2, learning_rate=0.01):\n",
    "    Z1 = np.dot(X, W1)\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2)\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    error = A2 - y_true\n",
    "    dW2 = np.dot(A1.T, error * sigmoid_derivative(Z2))\n",
    "    dW1 = np.dot(X.T, np.dot(error * sigmoid_derivative(Z2), W2.T) * sigmoid_derivative(Z1))\n",
    "    \n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2\n",
    "    \n",
    "    return W1, W2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "O cálculo é uma ferramenta essencial no desenvolvimento de redes neurais, sendo utilizado na otimização dos pesos, na modelagem das funções de ativação e na extração de características de sinais. Compreender esses conceitos matemáticos permite avançar na implementação e aprimoramento de modelos de aprendizado profundo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
